global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'
  pagerduty_url: 'https://events.pagerduty.com/v2/enqueue'

# Templates for notifications
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# The root route
route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  
  routes:
    # Critical alerts go to PagerDuty
    - match:
        severity: critical
      receiver: pagerduty-critical
      continue: true
      
    # Security alerts
    - match:
        team: security
      receiver: security-team
      
    # Database alerts
    - match:
        team: database
      receiver: database-team
      
    # Platform alerts
    - match:
        team: platform
      receiver: platform-team
      
    # Backend service alerts
    - match:
        team: backend
      receiver: backend-team
      
    # Claims team alerts
    - match:
        team: claims
      receiver: claims-team

# Receivers
receivers:
  - name: 'default'
    slack_configs:
      - channel: '#alerts'
        title: 'Sydney Health Alert'
        text: '{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}'
        send_resolved: true

  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ template "pagerduty.default.description" . }}'
        client: 'Sydney Health AlertManager'
        client_url: '{{ template "pagerduty.default.clientURL" . }}'
        details:
          firing: '{{ template "pagerduty.default.instances" .Alerts.Firing }}'
          resolved: '{{ template "pagerduty.default.instances" .Alerts.Resolved }}'
          num_firing: '{{ .Alerts.Firing | len }}'
          num_resolved: '{{ .Alerts.Resolved | len }}'

  - name: 'security-team'
    slack_configs:
      - channel: '#security-alerts'
        title: 'Security Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    email_configs:
      - to: 'security-team@sydneyhealth.com'
        headers:
          Subject: 'Security Alert: {{ .GroupLabels.alertname }}'

  - name: 'database-team'
    slack_configs:
      - channel: '#database-alerts'
        title: 'Database Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'platform-team'
    slack_configs:
      - channel: '#platform-alerts'
        title: 'Platform Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'backend-team'
    slack_configs:
      - channel: '#backend-alerts'
        title: 'Backend Service Alert - {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}Service: {{ .Labels.service }}\n{{ .Annotations.description }}{{ end }}'
        send_resolved: true

  - name: 'claims-team'
    slack_configs:
      - channel: '#claims-alerts'
        title: 'Claims Processing Alert'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        send_resolved: true
    email_configs:
      - to: 'claims-team@sydneyhealth.com'

# Inhibition rules
inhibit_rules:
  # Inhibit notifications for lower severity alerts if critical alert is firing
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
    
  # Inhibit node alerts if the whole cluster is down
  - source_match:
      alertname: 'ClusterDown'
    target_match:
      alertname: 'NodeDown'
    equal: ['cluster']